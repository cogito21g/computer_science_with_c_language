## 11. 병렬 프로그래밍

### 11.1 병렬 프로그래밍 모델

**이론**

병렬 프로그래밍 모델은 병렬 컴퓨팅 환경에서 작업을 수행하기 위한 프로그래밍 패러다임입니다. 이러한 모델은 병렬 처리를 위해 작업을 나누고, 처리된 결과를 통합하는 방법을 정의합니다. 병렬 프로그래밍 모델은 주로 다음과 같이 분류됩니다:

1. **공유 메모리 모델**:
    - 모든 프로세스나 스레드가 동일한 메모리 공간을 공유합니다.
    - 메모리 접근을 동기화하기 위해 뮤텍스, 세마포어, 모니터 등을 사용합니다.
    - 예: POSIX Threads, OpenMP

2. **분산 메모리 모델**:
    - 각 프로세스가 독립적인 메모리 공간을 가지며, 메시지 전달을 통해 통신합니다.
    - 데이터를 주고받기 위해 메시지 패싱을 사용합니다.
    - 예: MPI (Message Passing Interface)

3. **혼합 모델**:
    - 공유 메모리 모델과 분산 메모리 모델을 결합하여 사용합니다.
    - 노드 간에는 분산 메모리 모델을, 노드 내에서는 공유 메모리 모델을 사용하는 하이브리드 접근 방식입니다.
    - 예: MPI + OpenMP

### 공유 메모리 모델

공유 메모리 모델에서는 모든 프로세스나 스레드가 동일한 메모리 공간을 공유하여 데이터를 주고받습니다. 이 모델은 멀티스레드 프로그래밍에 많이 사용되며, 데이터를 공유하기 때문에 효율적인 통신이 가능합니다. 그러나 동기화 문제와 경합 상태(race condition)를 방지하기 위한 메커니즘이 필요합니다.

**예제 코드 (POSIX Threads, C)**:

```c
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>

#define NUM_THREADS 4
#define ARRAY_SIZE 1000

int array[ARRAY_SIZE];
int sum = 0;
pthread_mutex_t sum_mutex;

void* sum_array(void* arg) {
    int thread_part = *((int*)arg);
    int part_size = ARRAY_SIZE / NUM_THREADS;

    int start = thread_part * part_size;
    int end = (thread_part + 1) * part_size;

    int local_sum = 0;
    for (int i = start; i < end; i++) {
        local_sum += array[i];
    }

    pthread_mutex_lock(&sum_mutex);
    sum += local_sum;
    pthread_mutex_unlock(&sum_mutex);

    pthread_exit(0);
}

int main() {
    pthread_t threads[NUM_THREADS];
    int thread_args[NUM_THREADS];
    pthread_mutex_init(&sum_mutex, NULL);

    // 배열 초기화
    for (int i = 0; i < ARRAY_SIZE; i++) {
        array[i] = 1;
    }

    // 스레드 생성
    for (int i = 0; i < NUM_THREADS; i++) {
        thread_args[i] = i;
        pthread_create(&threads[i], NULL, sum_array, (void*)&thread_args[i]);
    }

    // 스레드 완료 대기
    for (int i = 0; i < NUM_THREADS; i++) {
        pthread_join(threads[i], NULL);
    }

    printf("Total sum: %d\n", sum);

    pthread_mutex_destroy(&sum_mutex);
    return 0;
}
```

**코드 설명**:
- `array[ARRAY_SIZE]` 배열은 합을 계산할 대상입니다.
- `sum` 변수는 배열 요소의 합을 저장합니다.
- `sum_mutex`는 `sum` 변수의 동기화를 위해 사용됩니다.
- `sum_array` 함수는 각 스레드가 배열의 부분 합을 계산하는 함수입니다. 각 스레드는 배열의 일부분을 처리하고, 결과를 `sum` 변수에 추가합니다.
- `main` 함수에서는 스레드를 생성하고, 각 스레드가 `sum_array` 함수를 실행하도록 합니다. 모든 스레드가 완료되면 최종 합을 출력합니다.

### 분산 메모리 모델

분산 메모리 모델에서는 각 프로세스가 독립적인 메모리 공간을 가지며, 메시지 전달을 통해 통신합니다. 이 모델은 MPI와 같은 라이브러리를 사용하여 구현됩니다. 프로세스 간의 통신은 네트워크를 통해 이루어지며, 데이터의 이동이 필요합니다.

**예제 코드 (MPI, C)**:

```c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

#define ARRAY_SIZE 1000

int main(int argc, char* argv[]) {
    int rank, size;
    int array[ARRAY_SIZE];
    int local_sum = 0, global_sum = 0;

    MPI_Init(&argc, &argv);                   // MPI 환경 초기화
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);     // 현재 프로세스의 순위 얻기
    MPI_Comm_size(MPI_COMM_WORLD, &size);     // 전체 프로세스 수 얻기

    if (rank == 0) {
        // 배열 초기화 (마스터 프로세스)
        for (int i = 0; i < ARRAY_SIZE; i++) {
            array[i] = 1;
        }
    }

    // 배열 부분을 각 프로세스로 브로드캐스트
    MPI_Bcast(array, ARRAY_SIZE, MPI_INT, 0, MPI_COMM_WORLD);

    // 각 프로세스가 자신의 부분 합 계산
    int part_size = ARRAY_SIZE / size;
    int start = rank * part_size;
    int end = (rank + 1) * part_size;

    for (int i = start; i < end; i++) {
        local_sum += array[i];
    }

    // 모든 부분 합을 마스터 프로세스로 집계
    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Total sum: %d\n", global_sum);
    }

    MPI_Finalize();  // MPI 환경 종료
    return 0;
}
```

**코드 설명**:
- `MPI_Init`: MPI 환경을 초기화합니다.
- `MPI_Comm_rank`: 현재 프로세스의 순위를 얻습니다.
- `MPI_Comm_size`: 전체 프로세스 수를 얻습니다.
- `MPI_Bcast`: 배열을 모든 프로세스에 브로드캐스트합니다.
- `MPI_Reduce`: 모든 프로세스의 부분 합을 마스터 프로세스로 집계합니다.
- `MPI_Finalize`: MPI 환경을 종료합니다.

### 혼합 모델

혼합 모델은 공유 메모리 모델과 분산 메모리 모델을 결합하여 사용하는 접근 방식입니다. 노드 간에는 분산 메모리 모델을 사용하고, 노드 내에서는 공유 메모리 모델을 사용하여 병렬 처리를 최적화합니다.

**예제 코드 (MPI + OpenMP, C)**:

```c
#include <mpi.h>
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>

#define ARRAY_SIZE 1000
#define NUM_THREADS 4

int main(int argc, char* argv[]) {
    int rank, size;
    int array[ARRAY_SIZE];
    int local_sum = 0, global_sum = 0;

    MPI_Init(&argc, &argv);                   // MPI 환경 초기화
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);     // 현재 프로세스의 순위 얻기
    MPI_Comm_size(MPI_COMM_WORLD, &size);     // 전체 프로세스 수 얻기

    if (rank == 0) {
        // 배열 초기화 (마스터 프로세스)
        for (int i = 0; i < ARRAY_SIZE; i++) {
            array[i] = 1;
        }
    }

    // 배열 부분을 각 프로세스로 브로드캐스트
    MPI_Bcast(array, ARRAY_SIZE, MPI_INT, 0, MPI_COMM_WORLD);

    // 각 프로세스가 자신의 부분 합 계산 (OpenMP 병렬화)
    int part_size = ARRAY_SIZE / size;
    int start = rank * part_size;
    int end = (rank + 1) * part_size;

    #pragma omp parallel for reduction(+:local_sum) num_threads(NUM_THREADS)
    for (int i = start; i < end; i++) {
        local_sum += array[i];
    }

    // 모든 부분 합을 마스터 프로세스로 집계
    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Total sum: %d\n", global_sum);
    }

    MPI_Finalize();  // MPI 환경 종료
    return 0;
}
```

**코드 설명**:
- `MPI_Init`: MPI 환경을 초기화합니다.
- `MPI_Comm_rank`: 현재 프로세스의 순위를 얻습니다.
- `MPI_Comm_size`: 전체 프로세스 수를 얻습니다.
- `MPI_Bcast`: 배열을 모든 프로세스에 브로드캐스트합니다.
- `MPI_Reduce`: 모든 프로세스의 부분 합을 마스터 프로세스로 집계합니다.
- `MPI_Finalize`: MPI 환경을 종료합니다.
- `#pragma omp parallel

 for reduction(+:local_sum) num_threads(NUM_THREADS)`: OpenMP를 사용하여 배열의 부분 합을 병렬로 계산합니다.

### 실습 과제

#### 과제 1: OpenMP와 MPI를 사용하여 배열의 최대값 찾기

**목표**:
- OpenMP와 MPI를 사용하여 배열의 최대값을 병렬로 찾는 프로그램을 작성합니다.

**해설**:
1. 배열을 초기화합니다.
2. 각 프로세스가 배열의 부분 최대값을 찾습니다.
3. MPI_Reduce를 사용하여 최종 최대값을 계산합니다.

**코드 예제**:

```c
#include <mpi.h>
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
#include <limits.h>

#define ARRAY_SIZE 1000
#define NUM_THREADS 4

int main(int argc, char* argv[]) {
    int rank, size;
    int array[ARRAY_SIZE];
    int local_max = INT_MIN, global_max = INT_MIN;

    MPI_Init(&argc, &argv);                   // MPI 환경 초기화
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);     // 현재 프로세스의 순위 얻기
    MPI_Comm_size(MPI_COMM_WORLD, &size);     // 전체 프로세스 수 얻기

    if (rank == 0) {
        // 배열 초기화 (마스터 프로세스)
        for (int i = 0; i < ARRAY_SIZE; i++) {
            array[i] = rand() % 1000;  // 0부터 999까지의 난수
        }
    }

    // 배열 부분을 각 프로세스로 브로드캐스트
    MPI_Bcast(array, ARRAY_SIZE, MPI_INT, 0, MPI_COMM_WORLD);

    // 각 프로세스가 자신의 부분 최대값 계산 (OpenMP 병렬화)
    int part_size = ARRAY_SIZE / size;
    int start = rank * part_size;
    int end = (rank + 1) * part_size;

    #pragma omp parallel for reduction(max:local_max) num_threads(NUM_THREADS)
    for (int i = start; i < end; i++) {
        if (array[i] > local_max) {
            local_max = array[i];
        }
    }

    // 모든 부분 최대값을 마스터 프로세스로 집계
    MPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Maximum value: %d\n", global_max);
    }

    MPI_Finalize();  // MPI 환경 종료
    return 0;
}
```

**코드 설명**:
- 배열을 초기화하고, 각 요소를 난수로 설정합니다.
- OpenMP를 사용하여 배열의 부분 최대값을 병렬로 계산합니다.
- reduction 절을 사용하여 각 스레드의 부분 최대값을 최종 최대값으로 집계합니다.
- MPI를 사용하여 각 프로세스의 부분 최대값을 집계하고, 최종 최대값을 계산합니다.
