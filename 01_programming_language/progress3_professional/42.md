## 10. 고성능 컴퓨팅 (HPC) 기법

### 10.3 MPI를 이용한 분산 컴퓨팅

**이론**

MPI(Message Passing Interface)는 분산 메모리 환경에서 프로세스 간의 통신을 위한 표준 API입니다. MPI는 주로 고성능 컴퓨팅(HPC)에서 사용되며, 여러 노드에 걸쳐 있는 여러 프로세스 간에 메시지를 주고받을 수 있습니다.

### MPI의 주요 구성 요소

1. **초기화 및 종료**:
    - `MPI_Init`: MPI 환경을 초기화합니다.
    - `MPI_Finalize`: MPI 환경을 종료합니다.

2. **통신**:
    - **점대점(Point-to-Point) 통신**: 두 프로세스 간의 통신
        - `MPI_Send`: 메시지를 보냅니다.
        - `MPI_Recv`: 메시지를 받습니다.
    - **집단(Collective) 통신**: 여러 프로세스 간의 통신
        - `MPI_Bcast`: 데이터를 모든 프로세스에 브로드캐스트합니다.
        - `MPI_Reduce`: 여러 프로세스에서 데이터를 모아 집계합니다.

3. **프로세스 정보**:
    - `MPI_Comm_size`: 통신 그룹의 크기를 반환합니다.
    - `MPI_Comm_rank`: 통신 그룹 내에서 현재 프로세스의 순위를 반환합니다.

### MPI 예제

간단한 MPI 예제를 통해 분산 컴퓨팅의 개념을 이해하겠습니다. 다음 예제는 MPI를 사용하여 배열의 합을 계산하는 방법을 보여줍니다.

**예제 코드 (C)**:

```c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

#define ARRAY_SIZE 1000

int main(int argc, char* argv[]) {
    int rank, size;
    int array[ARRAY_SIZE];
    int local_sum = 0, global_sum = 0;

    MPI_Init(&argc, &argv);                   // MPI 환경 초기화
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);     // 현재 프로세스의 순위 얻기
    MPI_Comm_size(MPI_COMM_WORLD, &size);     // 전체 프로세스 수 얻기

    if (rank == 0) {
        // 배열 초기화 (마스터 프로세스)
        for (int i = 0; i < ARRAY_SIZE; i++) {
            array[i] = 1;
        }
    }

    // 배열 부분을 각 프로세스로 브로드캐스트
    MPI_Bcast(array, ARRAY_SIZE, MPI_INT, 0, MPI_COMM_WORLD);

    // 각 프로세스가 자신의 부분 합 계산
    int part_size = ARRAY_SIZE / size;
    int start = rank * part_size;
    int end = (rank + 1) * part_size;

    for (int i = start; i < end; i++) {
        local_sum += array[i];
    }

    // 모든 부분 합을 마스터 프로세스로 집계
    MPI_Reduce(&local_sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Total sum: %d\n", global_sum);
    }

    MPI_Finalize();  // MPI 환경 종료
    return 0;
}
```

**코드 설명**:
- `MPI_Init`: MPI 환경을 초기화합니다.
- `MPI_Comm_rank`: 현재 프로세스의 순위를 얻습니다.
- `MPI_Comm_size`: 전체 프로세스 수를 얻습니다.
- `MPI_Bcast`: 배열을 모든 프로세스에 브로드캐스트합니다.
- `MPI_Reduce`: 모든 프로세스의 부분 합을 마스터 프로세스로 집계합니다.
- `MPI_Finalize`: MPI 환경을 종료합니다.

### 실습 과제

#### 과제 1: MPI를 사용하여 배열의 최대값 찾기

**목표**:
- MPI를 사용하여 배열의 최대값을 분산 컴퓨팅으로 찾는 프로그램을 작성합니다.

**해설**:
1. 배열을 초기화합니다.
2. 각 프로세스가 배열의 부분 최대값을 찾습니다.
3. MPI_Reduce를 사용하여 최종 최대값을 계산합니다.

**코드 예제**:

```c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

#define ARRAY_SIZE 1000

int main(int argc, char* argv[]) {
    int rank, size;
    int array[ARRAY_SIZE];
    int local_max = -1, global_max = -1;

    MPI_Init(&argc, &argv);                   // MPI 환경 초기화
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);     // 현재 프로세스의 순위 얻기
    MPI_Comm_size(MPI_COMM_WORLD, &size);     // 전체 프로세스 수 얻기

    if (rank == 0) {
        // 배열 초기화 (마스터 프로세스)
        for (int i = 0; i < ARRAY_SIZE; i++) {
            array[i] = rand() % 1000;  // 0부터 999까지의 난수
        }
    }

    // 배열 부분을 각 프로세스로 브로드캐스트
    MPI_Bcast(array, ARRAY_SIZE, MPI_INT, 0, MPI_COMM_WORLD);

    // 각 프로세스가 자신의 부분 최대값 계산
    int part_size = ARRAY_SIZE / size;
    int start = rank * part_size;
    int end = (rank + 1) * part_size;

    for (int i = start; i < end; i++) {
        if (array[i] > local_max) {
            local_max = array[i];
        }
    }

    // 모든 부분 최대값을 마스터 프로세스로 집계
    MPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Maximum value: %d\n", global_max);
    }

    MPI_Finalize();  // MPI 환경 종료
    return 0;
}
```

**코드 설명**:
- `MPI_Init`: MPI 환경을 초기화합니다.
- `MPI_Comm_rank`: 현재 프로세스의 순위를 얻습니다.
- `MPI_Comm_size`: 전체 프로세스 수를 얻습니다.
- `MPI_Bcast`: 배열을 모든 프로세스에 브로드캐스트합니다.
- `MPI_Reduce`: 모든 프로세스의 부분 최대값을 마스터 프로세스로 집계합니다.
- `MPI_Finalize`: MPI 환경을 종료합니다.

### 고급 MPI 기능

1. **비동기 통신**:
    - `MPI_Isend`: 비동기적으로 메시지를 보냅니다.
    - `MPI_Irecv`: 비동기적으로 메시지를 받습니다.
    - `MPI_Wait`: 비동기 통신이 완료될 때까지 기다립니다.

2. **집단 통신**:
    - `MPI_Scatter`: 데이터를 각 프로세스로 분산합니다.
    - `MPI_Gather`: 각 프로세스의 데이터를 모아서 하나의 프로세스로 수집합니다.
    - `MPI_Allgather`: 모든 프로세스가 데이터를 모아서 공유합니다.

3. **파생 데이터 타입**:
    - 사용자 정의 데이터 타입을 만들어 MPI에서 전송할 수 있습니다.
    - `MPI_Type_create_struct`: 사용자 정의 구조체를 생성합니다.
    - `MPI_Type_commit`: 사용자 정의 데이터 타입을 커밋합니다.

### 실습 과제

#### 과제 2: MPI를 사용한 행렬 곱셈

**목표**:
- MPI를 사용하여 두 행렬의 곱셈을 분산 컴퓨팅으로 수행하는 프로그램을 작성합니다.

**해설**:
1. 두 행렬을 초기화합니다.
2. 각 프로세스가 행렬의 일부분을 곱합니다.
3. MPI_Gather를 사용하여 최종 결과를 집계합니다.

**코드 예제**:

```c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

#define SIZE 100

void initialize_matrix(int matrix[SIZE][SIZE]) {
    for (int i = 0; i < SIZE; i++) {
        for (int j = 0; j < SIZE; j++) {
            matrix[i][j] = rand() % 100;
        }
    }
}

void print_matrix(int matrix[SIZE][SIZE]) {
    for (int i = 0; i < SIZE; i++) {
        for (int j = 0; j < SIZE; j++) {
            printf("%d ", matrix[i][j]);
        }
        printf("\n");
    }
}

int main(int argc, char* argv[]) {
    int rank, size;
    int A[SIZE][SIZE];
    int B[SIZE][SIZE];
    int C[SIZE][SIZE] = {0};

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    if (rank == 0) {
        initialize_matrix(A);
        initialize_matrix(B);
    }

    MPI_Bcast(B, SIZE*SIZE, MPI_INT, 0, MPI_COMM_WORLD);
    MPI_Scatter(A, SIZE*SIZE/size, MPI_INT, A[rank*SIZE/size], SIZE*SIZE/size, MPI_INT, 0, MPI_COMM_WORLD

);

    for (int i = 0; i < SIZE/size; i++) {
        for (int j = 0; j < SIZE; j++) {
            for (int k = 0; k < SIZE; k++) {
                C[i + rank*(SIZE/size)][j] += A[i + rank*(SIZE/size)][k] * B[k][j];
            }
        }
    }

    MPI_Gather(C[rank*SIZE/size], SIZE*SIZE/size, MPI_INT, C, SIZE*SIZE/size, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Resultant Matrix:\n");
        print_matrix(C);
    }

    MPI_Finalize();
    return 0;
}
```

**코드 설명**:
- `MPI_Init`: MPI 환경을 초기화합니다.
- `MPI_Comm_rank`: 현재 프로세스의 순위를 얻습니다.
- `MPI_Comm_size`: 전체 프로세스 수를 얻습니다.
- `MPI_Bcast`: 행렬 B를 모든 프로세스에 브로드캐스트합니다.
- `MPI_Scatter`: 행렬 A를 각 프로세스로 분산합니다.
- 각 프로세스가 자신에게 할당된 행렬의 일부분을 곱합니다.
- `MPI_Gather`: 각 프로세스의 결과를 하나의 행렬로 모읍니다.
- `MPI_Finalize`: MPI 환경을 종료합니다.
